\documentclass[a4paper,atbegshi,chapter,itemph,hidelinks,14pt]{oblivoir}
\usepackage{fapapersize}
\usefapapersize{210mm,297mm,30mm,*,30mm,32mm}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{braket,hyperref,nicematrix}
\usepackage{euler,enumitem,mdframed,ob-chapstyles}
\chapterstyle{chappell}
\setlist{nosep}
\title{계산과학2 (1)}
\author{노현민-김태원 조}
\date{\today}
\begin{document}
\maketitle\pagestyle{empty}\newpage
\tableofcontents
\chapter{개요}
\emph{역양자화\footnotesize dequantization}는 특정 양자 기계학습 알고리즘의
고전 기계학습 판본을 오직 다항 감속과 함께 제시하는 기법이다. 

양자 기계학습과 양자 기계학습이 취한다고들 하는 계산적인 우위는 HHL 알고리즘에
바탕한다. 그러나 혹자가 보기에 여기에는 몇 가지 치명적인 가정이 존재하며 HHL
알고리즘에 따라 발전한 여타 양자 기계학습 알고리즘도 마찬가지였다.

이후 탕{\tiny Ewin Tang}은 양자 추천 알고리즘 하나를 오직 다항 감속과 함께
역양자화한다. 양자 위상 추정과 같은 양자 기법과 종래의 선형 대수 기법 간의
긴밀한 관계를 포착한 결과였다. 이 결과의 핵심 함의는 양자 선형대수 알고리즘을
효율적으로 역양자화할 수 있으니 ``양자''라는 용어의 근본 개념과 용래를 재고해야
한다는 것이다. 

그러나 이후 일군의 연구자들이 주장하기를 역양자화는 양자계에서 비롯하는
데이터에 적용하기 어려운 기법이다. 양자 실험으로 취한 양자 데이터에 대해 양자
기계학습 알고리즘은 부인할 수 없는 속도 증진을 일으키기 때문이다. 따라서 
실질적으로 QRAM을 비롯하여 실험 양자 데이터를 개발 및 연구해서 이런 의문을 
해소할 수 있다는 것이다. 

그런데도 양자 선형대수와 고전 선형대수의 알고리즘적인 관계는 아직 미지의
영역이다. 본고는 상술한 과정을 일별하고 역양자화 기법의 의의를 탕이 최근
제안한 구체적인 문제로 명시한다. 
\chapter{HHL}
\section{HHL 알고리즘}
에르미트 $N\times N$ 행렬 $A$, 단위 벡터 $\pmb{b}$가 있다고 하자. 이때 에르미트
행렬이란 켤레 전치가 자기 자신과 같은 복소정방행렬이다. 연립일차방정식의
해를 구하는 문제는 $A\pmb{x}=\pmb{b}$를 만족하는 $\mathbf{x}$를 찾는 것이다. 우선
$\pmb{b}$를 아래와 같은 양자 상태로 나타낸다.
\[
  \ket{b} = \sum_{i=1}^N b_i\ket{i}
\]
이에 해밀토니언 시뮬레이션이라는 기법을 사용해 $e^{-iAt}$를 상이한 시간 $t$의 중첩
$\ket{b}$에 적용한다. $e^{-iHt}$는 계의 총 에너지에 대응하는 연산자인 해밀토니언
$H$의 시간 $t$에 따른 이상적인 변화를 말하는데, 해밀토니언 시뮬레이션은 최대
오류 $\epsilon$에 대해 $\|U-e^{-iHt}\|\leq\epsilon$인 유니타리 변환 $U$에 
근사하는 알고리즘을 찾는 문제다. 그리하여 행렬 $A$를 해밀토니언으로 두고
시간 $t$에 따른 복소평면상의 매끄럽고 이상적인 변화 결과 $e^{-iAt}$를 $\ket{b}$에
적용하는 것, 다시 말해 $\ket{b}$를 $A$의 고유벡터 $u_j=e^{-iAt}$로 분해하여 이에
대응하는 고윳값 $\lambda_j$를 취하는 것이 HHL 알고리즘의 핵심 기법이다.

이후 $\ket{b}=\sum_{j=1}^N\beta_j\ket{u_j}$에 대해 연립일차방정식은 아래와
같은 꼴을 보인다.
\[
  \sum_{j=1}^N\beta_j\lambda_j\ket{u_j}
\]
물론 여기서 $\ket{u_j}$가 유니타리라는 보장은 없다. 따라서 이 과정에서 실패가
발생할 수 있다. 이처럼 $e^{-iAt}$를 고유벡터 $\ket{u_j}$ 삼아서 고윳값
$\lambda_j$를 추정하는 기법을 위상 추정이라고 부르는데, 위상 추정은 유니타리(라서
단위율을 지니는) 연산자를 요구하기 때문이다. 

아무튼 성공하면 $\lambda_j$ 항목을 역산해서 $\ket{x}$에 상응하는
상태를 취할 수 있다.
\[
  \sum_{j=1}^N\beta_j\lambda_j^{-1}\ket{u_j}=A^{-1}\ket{b}=\ket{x}
\]
이와 같은 역산에서 결정적인 역할을 맡는 것이 바로 조건수 $k$다. $k$는 $A$의 최대
고윳값과 최소 고윳값의 비율이라고 보면 된다. 조건수가 크면 역산을 수행하기
어렵고 그만큼 결과가 불안정하다. 이때 HHL은 $A$의 특잇값, 다시 말해 고윳값의
제곱근이 $1/k$와 $1$ 가운데 있다고 아래처럼 가정한다.
\[
  k^{-2}I\leq A^{\dagger}A \leq I
\]
이러한 가정 아래 실행시간은 $k^2\log(N)/\epsilon$에 비례한다. 따라서 HHL은
$k$와 $1/\epsilon$이 $\textrm{poly}\log(N)$일 때 지수적인 속도 증진을 보인다.
\section{HHL의 `세부 조항'}
HHL 알고리즘의 로그시간 해법을 개괄했다. HHL 알고리즘은 이후 양자기계학습이라는
분야 자체의 초석이자 양자기계학습이 지수적인 속도 증진을 취한다는 주장의 근거로
자리 잡았다. 이에 애론슨{\tiny Scott Aaronson}은 HHL의 결정적인 `세부 조항'{\tiny
fine print}들을 지적한다.
\hfill\break
\begin{enumerate}[label=(\alph*)]
  \item 벡터 $\pmb{b}$를 재빠르게 양자컴퓨터의 매모리에 적재해야 한다. 그래야
    양자 상태 $\ket{b}=\sum_{i=1}^n b_i\ket{i}$를 마련할 수 있다. 이론상으로는
    이른바  ``양자 RAM''을 사용하면 그만이다. 중첩을 통해 $b_i$를 전부 한꺼번에
    읽는 장치다. 이런 QRAM이 있더라도 $\pmb{b}$는 대체로 균일해야 한다. 즉
    어떤 $b_i$가 여타 성분보다 지나치게 크면 안 된다. $\pmb{b}$가 균일하지
    않다면 QRAM이 $\pmb{b}$의 큰 성분에 대한 포인터를 지녀야 한다. 아무튼
    $\ket{b}$를 마련하려면 어떤 상수 $c$에 대해 $n^c$ 스텝이 소요된다.
  \item 양자컴퓨터가 여러 $t$ 값에 대해 $e^{-iAt}$ 꼴의 유니타리 변환을 적용할
    수 있어야 한다. 이때 $A$가 희소행렬인 경우처럼 특수한 상황이 아니라면
    $e^{-iAt}$를 적용하는 작업에만 $n^c$ 시간이 든다.
  \item $A$가 가역행렬이어야 할 뿐 아니라 `가역하기 쉬운' 행렬이어야 한다.
    앞서 언급했듯이 $k$가 $n^c$꼴로 늘어나면 지수적인 속도 증진이 사라진다.
  \item $\pmb{x}$를 작성하는 작업에만 $n$ 스텝이 소요된다.
    HHL은 $\pmb{x}$ 자체가 아니라 $\log_2n$ 큐비트로 구성된 양자 상태 $\ket{x}$를
    내놓는데 이는 $\pmb{x}$의 성분을 진폭으로 근사하여 나타낸다. 이후
    양자컴퓨터는 $\ket{x}$를 측정해 $\pmb{x}$에 관한 통계적인 정보를 취할 수 있다.
    다만 특정 성분 $x_i$에 관한 정보를 취하려면 알고리즘을 대략 $n$번 반복해야
    한다.
\end{enumerate}
\hfill\break
애론슨의 의문은 HHL의 지수적인 속도 증진을 위해 이들 조건을 전부 지킬 일이
많겠냐는 것이다. 

물론 HHL은 여타 양자 기계학습 알고리즘을 작성하기 위한 템플릿으로 기능할 수 있다.
$\ket{b}$의 마련 방법, $e^{-iAt}$의 적용 방법, $\ket{x}$의 측정 방법 같은 부분
물음을 제시한 셈이다. 실상 $A\pmb{x}=\pmb{b}$의 해를 구하는 로그시간 실질적인
방법보다는 $A\pmb{x}= \pmb{b}$를 양자 시뮬레이션하여 속도 증진을 이룰 수 있다는
쪽이 HHL의 요지에 가깝다. 
\chapter{추천}
\end{document}
