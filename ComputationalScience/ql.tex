%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                   %
%                      JAMI - LaTeX format                          %
%                                                                   %
%     E-mail address  : ryoocs@hnu.kr                               %
%                                                                   %
%                                                                   %
%                                                                   %
%   We are accepting manuscripts with AMSTeX or LaTeX versions      %
%   through E-mail.                                                 %
%                                                                   %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[10pt,twoside,reqno]{amsart} %For LaTeX2e users

%\documentstyle[10pt,twoside]{amsart} %For old version of Latex users

\allowdisplaybreaks
\usepackage{amsmath,amstext,amssymb,braket}
\usepackage{graphicx,forest}
\usetikzlibrary{arrows.meta}

\textwidth 12.2 cm
\textheight 19.3 cm
\oddsidemargin 1.2cm
\evensidemargin 1.2cm
\calclayout

\setcounter{page}{1}
%
\makeatletter
\renewcommand{\@seccntformat}[1]{\bf\csname the#1\endcsname. }
\renewcommand{\section}{\@startsection{section}{1}
   \z@{.7\linespacing\@plus\linespacing}{.5\linespacing}
   {\normalfont\upshape\bfseries\centering}}
%\@addtoreset{thm}{section} \@addtoreset{rem}{section}
\renewcommand{\@biblabel}[1]{\@ifnotempty{#1}{#1.}}
\makeatother
%
\renewcommand{\refname}{\sc References}
%
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{exa}[thm]{Example}
\newtheorem{defn}[thm]{Definition}
\newtheorem{rem}{Remark}[section]

\def\pn{\par\noindent}


\begin{document}

\leftline{\scriptsize \it  J. Appl. Math. {$\&$}
Informatics Vol. {\bf 4x}(2023), No. x, pp. xx - xx   }
\vspace{-0.15cm}
%\leftline{\footnotesize Website: http://www.jami.or.kr}
\leftline{\footnotesize https://doi.org/10.14317/jami.201x.xxx}



\vspace{1.3cm}

\title
{ %Full title of the paper
Variation in norms of quantum input models$^\dagger$  
}

\author
{ %Firstname Lastname, Firstname Lastname
  Hyeonmin Roh$^*$, Taewon Kim$^{**}$ 
}

\thanks{ {\scriptsize  Received     ,     .  Revised ,   .    Accepted     ,   . \enskip $^*$Corresponding author.}\\
\indent{\scriptsize $^\dagger$%funding-------------------------------------------------------------------
{Please add: ``This research received no external funding'' or ``This research was funded by NAME OF FUNDER grant number XXX.'' and  and ``The APC was funded by XXX''. Check carefully that the details given are accurate and use the standard spelling of funding agency names, any errors may affect your future funding.}
Example:This work was supported by the research grant of the University}\\
\indent{\scriptsize $\copyright$ 2023 KSCAM}}


\maketitle


\begin{abstract}
  The main goal of this paper is to formulate a quantum data structure that
  prevents the existence of efficient classical counterparts of Quantum machine
  learning algorithms based upon the data structure or input assumption. Such
  existence and prevention have been an open problem in the field of quantum
  machine learning theory, and provide some general properties regarding the
  problem. %not exceed 160 words and must contain Main Facts 

\vskip 0.4 true cm


\noindent
 AMS Mathematics Subject Classification : 65H05, 65F10.

\noindent 
{\it Key words and phrases } : %keyword 1, keyword 2, keyword 3.
Nonlinear equation, three-step iterative method, multi-step iterative method.

\end{abstract}


\pagestyle{myheadings}
\markboth{\centerline {\scriptsize  the names of the authors  }}
         {\centerline {\scriptsize  title of paper  }}


\bigskip
\section{Introcution}
Quantum machine learning (QML) is a interdisciplinary field of study subsequent
to HHL algorithm \cite{harrow2009} that approximately solves a system of linear
equations in a logarithmic time. However, there have been serious critiques 
\cite{aaronson2015} on input model assumptions or quantum data structures
utilized in QML algorithms. Furthermore, Tang \cite{tang2019} introduced 
\emph{dequantization}, a method that provides efficient classical counterparts
on classical data for QML algorithms by randomized-linear algebraic exploitations
of quantum-advantageous assumptions. Since then, preventing dequantization
have been one of open problems \cite{tang2023} in QML theory.

%All recognized QML techniques rooted in linear algebra fall under the umbrella
%of Quantum Singular Value Transformation (QSVT), serving as a unifying framework
%for quantum algorithms. QSVT is categorized based on the assumptions of the input
%models. These assumptions revolve around whether the inputs for QML are sparse or
%low-rank. While sparse-access input models are known for providing exponential
%speedups, dequantization targets low-rank input models where classical data, 
%without strict limitations, can be applied.

Just like ML, input data models are commonly classical for QML. To make classical
data compatible with QML algorithms, such data need to be efficiently transformed
into quantum states. Simple solution is to assume the existence of Quantum 
Random Access Memory (QRAM), a quantum counterpart to classical RAM, capable of
storing $n$ bits of data and querying these data in superposition within a time
complexity of polylog$(n)$. Dequantization essentially involves
mimicking QRAM for classical computers by assuming an input model of sampling
and query access to a vector. This assumption allows for a fair comparison
between quantum and classical machine learning methodologies.

The outcomes of dequantization establish a boundary for our comprehension of QML
algorithms and their constraints. Consequently, a significant unresolved issue
in QML pertains to identifying alternative methods for constructing data 
structures that prevent dequantization. This issue is the focal point, 
considering its fundamental unit termed 'Block-encoding.' The objective is to
formally define an alternative data structure implicitly suggested by Kerenidis
and Prakash, and Chakraborty, Gily√©n, and Jeffery, and provid general exposition
to the question of what properties let QML algorithms to be dequantized.

\section{Nomenclature}
\section{Quantum accesibble classical data structure}
Regarding QML and dequantization, notion such as Quantum accesibble classical
memory or Quantum Read-Only Memory (QROM \cite{babbush2018}) are termed simply
`QRAM' because most data used in ML are classical. 

\begin{defn}[\cite{jaques2023} Definition 1]
  For a table of data $T\in\{0,1\}^N$, 
  QRAM is a collection of unitaries $U_Q(T)$,
  such that for all states $\ket{i}$ in the computational basis
  where $0\leq i\leq N-1$, 
  \[
    U_Q(T)\ket{i}\ket{0} = \ket{i}\ket{T_i}.
  \]
\end{defn}
Note that $U_Q(T)$ is unitary. By linearity, number of qubits during access
by superposition is $\lceil\log^N\rceil$. Hence, assume that query of the
form $\ket{i}\ket{0}\rightarrow\ket{i}\ket{T_i}$ requires 
$\mathcal{O}(\textrm{polylog})$ time. Following is an input model or a data
structure upon such QRAM, employed in quantum recommendation algorithm.
\begin{thm}[\cite{kerenidis2017} Theorem 15]
  Let $A\in\mathbb{R}^{m\times n}$ be a matrix. Let $(i,j,A_{ij})$ be 
  entries arriving in the system in a arbitrary order, and $w$ be the
  number of entries already in the system. Then, there exists a data structre
  to store the matrix $A$ with following properties:
  \begin{enumerate}
    \item The size of the data structure is $\mathcal{O}(w\log^2(mn))$.
    \item The time to store a new entry $(i,j,A_{ij})$ is 
      $\mathcal{O}(\log^2(mn))$.
    \item Corresponding to the rows of the matrix currently stored,
      a quantum algorithm that has quantum access to the data structure
      can perform the mapping 
      \[
        \widetilde{U}:\ket{i}\ket{0}\rightarrow\ket{i}\ket{A_i},
      \]
      and for $\widetilde{A}\in\mathbb{R}^m$ with entries 
      $\widetilde{A_i}=\|A_i\|$ and $j\in[n]$,
      \[
        \widetilde{V}:\ket{0}\ket{j}\rightarrow\ket{\widetilde{A}}\ket{j}.
      \]
      This quantum algorithm takes $\textrm{\normalfont polylog}(mn)$ time.
  \end{enumerate}
\end{thm}
This data structure is an array of $m$  binary trees. The value
stored at the root is $\|A_i\|^2$ for $i\in[m]$, and depth of each tree is
at most $\lceil\log n\rceil$. Here, dequantization questions the assumption
of `\emph{qauntum access} to the data structure that can efficiently handle
classical inputs' and provides a comparison equalizer. Goal is to classically
construct identical tree with only a polynomial slowdown. 
\begin{defn}[\cite{tang2023} Definition 4.1]
  For all $i\in[n]$, if we can query for $v(i)$, we have \emph{query access}
  to a vector $v\in\mathbb{C}^n$, denoted by $Q(v)$. For all 
  $(i,j)\in[m]\times[n]$, if we can query for $A_{ij}$, we have $Q(A)$ to
  a matrix $A\in\mathbb{C}^{m\times n}$. Time cost of such query is denoted
  by $q(v)$ and $q(A)$, respectively.
\end{defn}
\begin{defn}[\cite{tang2023} Definition 4.2]
  For a vector $v\in\mathbb{C}^n$, we have \emph{sampling and query access} to
  $v$, denoted by $SQ(v)$, if we can:
  \begin{enumerate}
    \item have query access to $v$;
    \item obtain independent samples $i\in[n]$ following the distribution
      $\mathcal{D}_v\in\mathbb{R}^n$ with $\mathcal{D}_v(i):=|v(i)|^2/\|v\|^2$;
    \item have query access to $\|v\|$.
  \end{enumerate}
  Cost of entry querying, index sampling, norm querying, are denoted as
  $q(v),s(v),$ and $n(v)$, respectively. Also, let $sq(v):=\max(q(v),s(v),n(v))$.
\end{defn}
Samples obtained from sampling and query access are analogue to the quantum
state $\ket{v} := 1/\|v\|\sum v_i\ket{i}$ in the computational basis. Such
sampling and query access may be generalized by some oversampling rate.
\begin{defn}
  For $v\in\mathbb{C}^n$ and $\phi\geq1$, we have
  $\widetilde{v}\in\mathbb{C}^n$ if $\|\widetilde{v}\|^2=\phi\|v\|^2$ and
  $|\widetilde{v}_i|^2\geq |v_i|^2$ for all $i\in[n]$.
\end{defn}
\begin{defn}[\cite{tang2023} Definition 4.3]
  For $v\in\mathbb{C}^n$ and $\phi\geq1$, if $Q(v)$ and 
  $SQ(\widetilde{v})$ for $\widetilde{v}\in\mathbb{C}^n$, we have 
  $\phi$-\emph{oversampling and query access to $v$} or $SQ_{\phi}(v)$. Also,
  \begin{align*}
    s_{\phi}(v) := s(\widetilde{v}),\;
    q_{\phi}(v) := q(\widetilde{v}),\;
    n_{\phi}(v) := n(\widetilde{v}),\;
    sq_{\phi}(v) := \max(s_{\phi}(v),q_{\phi}(v),n_{\phi}(v)).
  \end{align*}
\end{defn}

\begin{figure}[h]
  \begin{forest}
    for tree={
      edge={-{Stealth[]}},
    }
      [$\|\widetilde{A}\|^2$
        [$\|\widetilde{A_1}\|^2$
        [$\|\widetilde{A_{11}}\|^2+\|\widetilde{A_{12}}\|^2$
          [$\|\widetilde{A_{11}}\|^2$
            ]
            [
            $\|\widetilde{A_{12}}\|^2$
            ]
          ]
          [$\|\widetilde{A_{13}}\|^2+\|\widetilde{A_{14}}\|^2$
          [$\|\widetilde{A_{13}}\|^2$
            ]
            [$\|\widetilde{A_{14}}\|^2$
            ]
          ]
        ]
        [$\|\widetilde{A_2}\|^2$
        [$\|\widetilde{A_{21}}\|^2+\|\widetilde{A_{22}}\|^2$
          [$\|\widetilde{A_{21}}\|^2$
            ]
            [$\|\widetilde{A_{22}}\|^2$
            ]
          ]
          [ $\|\widetilde{A_{23}}\|^2+\|\widetilde{A_{24}}\|^2$
          [$\|\widetilde{A_{23}}\|^2$
            ]
            [$\|\widetilde{A_{24}}\|^2$
            ]
          ]
        ]
      ]
  \end{forest}
  \caption{example of a $\phi$-sampling and qeury access data structure}
\end{figure}
Note that $SQ_{\phi}(v)$ constructs an identical tree structure to the one
from QRAM. So, we can dequantize whenever QML algorithm relies on QRAM
based data structure. However, there have been variations for such definitions
of QRAM, which might prevent dequantization.
\section{QRAM, data structures and Block-encoding}
Regarding QML and dequantization, `QRAM' denotes a \emph{quantum accesibble
classical memory}.
\begin{defn}[\cite{jaques2023} Definition 1]
  is a collection of unitaries $U_{\textrm{QRAM}}(T)$, where 
  $T\in\{0,1\}^n$ is a table of data, such that for all states $\ket{i}$
  in the computational basis, $0\leq i\leq n-1$,
  \[
    U_{\textrm{QRAM}}(T)\ket{i}\ket0=\ket{i}\ket{T_i}_.
  \]
\end{defn}
The following is a data structure employed in the quantum recommendation 
algorithm, which happens to be the first QML algorithm to undergo the 
process of dequantization.
\begin{thm}[Kerenidis 2017, Theorem 15]
  Let $A \in \mathbb{R}^{m \times n}$ be a matrix with $A_{ij} \in \mathbb{R}$ representing
  the entry at the $i$-th row and the $j$-th column. Suppose $w$ is the count of entries that
  have already been received in the system, where entries $(i, j, A_{ij})$ arrive in an arbitrary order.
  In such a scenario, there exists a data structure for storing the matrix $A$ with the following properties:

  \begin{enumerate}
    \item The data structure's size is $\mathcal{O}(w \log^2(mn))$.
    \item When provided with entries $(i, j, A_{ij})$ in an arbitrary order, the time required to store them
       is $\mathcal{O}(\log^2(mn))$.
    \item A quantum algorithm exists that can perform the following mappings in $\textrm{\normalfont polylog}(mn)$ time
       for $i \in [m]$ and $j \in [n]$:
       \begin{align*}
        \widetilde{U} & : \ket{i}\ket{0} \mapsto \ket{i}\ket{A_i},\\
        \widetilde{V} & : \ket{0}\ket{j} \mapsto \ket{\widetilde{A}}\ket{j},
       \end{align*}
       where $\widetilde{A} \in \mathbb{R}^m$ has entries $\widetilde{A_i} = \|A_i\|$.
  \end{enumerate}
\end{thm}
\begin{proof}
  The data structure comprises an array of $m$ binary trees, denoted as $B_i$ 
  for $i \in [m]$. When a new entry $(i, j, A_{ij})$ arrives, a leaf node for
  $j$ in tree $B_i$ is created if it doesn't already exist, and updated if it
  does. The depth of each tree $B_i$ is at most $\lceil\log n\rceil$. An internal
  node $v$ within $B_i$ stores the sum of the values of all the leaves in the
  subtree rooted at $v$, which is essentially the sum of the square amplitudes
  of the entries of $A_i$ in that subtree. Consequently, the value stored at th
  e root of each tree is $\|A_i\|^2$. 
  \begin{enumerate}
    \item The memory required for this data structure is bounded by
      $\mathcal{O}(w\log^2 mn)$. For each entry $(i, j, A_{ij})$, at most
      $\lceil\log n\rceil$ new nodes are added, and each node necessitates
      $\mathcal{O}(\log mn)$ bits.
    \item The time required to store an entry $(i, j, A_{ij})$ is
      $\mathcal{O}(\log^2 mn)$. The insertion algorithm involves at most 
      $\lceil \log n\rceil$ updates to the data structure, and each update takes
      $\mathcal{O}(\log mn)$ time to retrieve the address of the updated node.
    \item The amplitudes stored in the internal nodes of $B_i$ are utilized
      to apply a sequence of conditional rotations to the initial state
      $\ket0^{\lceil\log n\rceil}$ to obtain $\ket{A_i}$. Furthermore, it's
      worth noting that the amplitudes of the vector $\widetilde{A}$ are equal
      to $\|A_i\|$, and the values stored in the roots of the trees $B_i$ are
      equal to $\|A_i\|^2$. Consequently, a similar construction for the $m$
      roots allows us to efficiently perform the unitary $\widetilde{V}$.
  \end{enumerate}
\end{proof}
\begin{thm}[Chakraborty, 2018, Theorem 1]
  Consider a matrix $A \in \mathbb{R}^{m \times n}$ with entries $A_{ij} \in
  \mathbb{R}$ representing the element at the $i$-th row and the $j$-th column.
  Let $w$ be the count of non-zero entries in $A$. Then, there exists a data
  structure of size $\mathcal{O}(w \log^2(mn))$ that, when provided with entries
  $(i, j, A_{ij})$ in any order, stores them such that the time required to store
  each entry of $A$ is $\mathcal{O}(\log(mn))$.

  Once this data structure has been initialized with all non-zero entries of $A$,
  there exists a quantum algorithm that can perform the following mappings with
  $\epsilon$-precision in $\mathcal{O}(\textrm{\normalfont 
  polylog}(mn/\epsilon))$ time:
  \begin{align*}
    \widetilde{U} & : \ket{i}\ket{0} \mapsto \frac{1}{\|A_{i \cdot}\|}
    \sum_{j=1}^n A_{i,j}\ket{j} = \ket{i, A_i}, \\
    \widetilde{V} & : \ket{0}\ket{j} \mapsto \frac{1}{\|A\|_F}
    \sum_{i=1}^m\|A_{i,\cdot}\|\ket{i}\ket{j} = \ket{\widetilde{A}, j},
  \end{align*}

  where $\ket{A_{i,\cdot}}$ is the normalized quantum state corresponding to
  the $i$-th row of $A$, and $\ket{\widetilde{A}}$ is a normalized quantum state
  such that $\braket{i|\widetilde{A}} = \|A_{i,\cdot}\|$, which represents the
  norm of the $i$-th row of $A$.

  Notably, when given a vector $\pmb{v} \in \mathbb{R}^{m \times 1}$ stored in
  this data structure, it is possible to generate an $\epsilon$-approximation of
  the superposition $\sum_{i=1}^m \frac{v_i}{\|\pmb{v}\|}\ket{i}$ with a
  computational complexity of $\textrm{\normalfont polylog}(m/\epsilon)$.
\end{thm}
\begin{proof}
\end{proof}
\begin{thm}[Kerenidis, Prakash, 2020, Theorem IV.2]
  Let $A\in\mathbb{R}^{m\times n}$ and $M=\max_{i\in[m]}\|a_i\|^2$. There is an
  efficient data structure for storing matrix entries $(i,j,a_{ij})$ such that
  access to this data structure allows a quantum algorithm to implement the
  following unitary in time $\widetilde{\mathcal{O}}(\log(mn))$.
  \[
    U\ket{i,0^{\lceil\log(n+1)\rceil}}=\ket{i}\frac{1}{\sqrt{m}}
    \left(\sum_{j\in[n]}a_{ij}\ket{j}+(m-\|a_i\|^2)^{1/2}\ket{n+1}\right)
  \]
\end{thm}
\begin{proof}
\end{proof}
\begin{thm}
  Thm on dequantizaion / or expansion of dat structs above.
\end{thm}
\begin{proof}
\end{proof}
\begin{cor}
  \textbf{Midway main Results}
\end{cor}
\begin{proof}
\end{proof}
\section{Block-encoding}

In order to appreciate powers of these alternatives, we introduce the notion
of block-encoding, which was devised as a solution to the problem of
Hamiltonian simulation. Hamiltonian simulation, one of the original motivations
for designing practical quantum computers, may stated as follows: For time
evolution of the wave function $\ket{\psi(t)}$ governed by the Schr{\"o}dinger
equation, that is,
\[
  i\hslash\frac{\textrm{d}}{\textrm{dt}}\ket{\psi(t)}
  = H(t)\ket{\psi(t)}
\]
the Hamiltonian, an operator with units of energy, is $H(t)$. Hamiltionian
simulation is a problem of designing quantum circuit or unitary matrix $U$
consisting of $\textrm{poly}(n,t,1/\epsilon)$ gates such that $\|U-{e^{iHt}}\|\leq
\epsilon$. % Childs - lecture notes
The cost of Hamiltonian simulation depends on the number of qubits
$n$, evolution time $t$, target error $\epsilon$, and access models of
Hamiltonian $H$. While acheiving optimal Hamiltonian simulation by the
process called `Qubitization', Low and Chuang (2017) defined a \emph{standard-form
encoding}, a primitive statement of \emph{block-encoding}. Basically, 
qubitization is a technique of representing Hermitian or subnormalized matrix
as the top-left block of a unitary matrix, that is;
\[
  U=\begin{bmatrix}A/\alpha &\cdot \\ \cdot &\cdot \end{bmatrix}
\]
where $\cdot$ denotes arbitrary elements of $U$. 
\begin{defn}[Block-encoding] For $A\in\mathbb{C}^{n\times m}, \alpha,\epsilon
  \in\mathbb{R}_{+}$ and $a\in\mathbb{N}$, $(s+a)$-qubit unitary $U$ is an
  $(\alpha,a,\epsilon)$-block-encoding of $A$ if
  \[
    \|A-\alpha(\bra0^{\otimes a}\otimes I)U(\ket0^{\otimes a}\otimes I)\|\leq
    \epsilon.
  \]
\end{defn}
For $n,m\leq 2^s$ we may define an embedding matrix $A_e\in
\mathbb{C}^{2^s\times 2^s}$ such that the top-left block of $A_e$ is $A$ and
all other entries are $0$.

\begin{thm}[Chakraborty, 2019, Lemma 25]
  Let $A\in\mathbb{C}^{m\times n}$.
  \begin{enumerate}
    \item Fix $p\in [0,1]$. If $A^{(p)}$ and $(A^{(1-p)})^{\dagger}$ are both
      stored in quantum-accessible data structures, then there exist
      unitaries $U_R$ and $U_L$ that can be implemented in time
      $\mathcal{O}(\textrm{\normalfont polylog}(mn/\epsilon))$ such that
      $U_R^{\dagger}U_L$ is a $(\mu_p(A), \lceil\log(n+m+1)\rceil, 
      \epsilon)$-block-encoding of $\bar{A}$.
    \item On th other hand, if $A$ is stored in a quantum-accessible data
      structure, then there exist unitaries $U_R$ and $U_L$ that can
      be implemented in time $\mathcal{O}(\textrm{\normalfont polylog}(mn/\epsilon))$ such
      that $U_R^{\dagger}U_L$ is a $(\|A\|_F, \lceil\log(m+n)\rceil,
      \epsilon)$-block-encoding of $\bar{A}$.
  \end{enumerate}
\end{thm}
\begin{proof}
  For $j\in[m]$, we difne $\ket{\psi_j}$ and $\ket{\phi_j}$ as follows.
  \begin{align*}
    \ket{\psi_j} &= \frac{\sum_{k\in[n]}A^p_{j,k}\ket{j,m+k}}{\sqrt{s_{2p}}(A)}+
    \sqrt{1-\frac{\sum_{k\in[n]}A^{2p}_{j,k}}{s_{2p}(A)}}\ket{j,n+m+1}\\
    \ket{\phi_j} &= \frac{}{} \cdots
  \end{align*}
\end{proof}
\begin{defn}
  singular value decomposition
\end{defn}
\begin{thm}[Kerenidis, Prakash, 2020, Theorem IV.4]
\end{thm}
\begin{proof}
\end{proof}

\begin{thm}
  \textbf{Main Results}
\end{thm}
\begin{proof}
\end{proof}
\section{Examples and Applications}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
%                                             %
%          Text for Introduction              %
%                                             %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                             %
%                                             %
%          Text for main results              %
%                                             %
%                                             %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\bigskip
{\bf Conflicts of interest} : {Declare conflicts of interest or state ``The authors declare no conflict of interest.'' Authors must identify and declare any personal circumstances or interest that may be perceived as inappropriately influencing the representation or interpretation of reported research results.} 

%% Optional-------------------------------------------------------------
\bigskip
{\bf Data availability} : {In this section, please provide details regarding where data supporting reported results can be found, including links to publicly archived datasets analyzed or generated during the study. If the study did not report any data, you might add ``Not applicable'' here.} 
%-----------------------------------------------------------------------

%% Optional-------------------------------------------------------------
\bigskip
{\bf Acknowledgments} : {In this section you can acknowledge any support given which is not covered by the author contribution or funding sections.}
%-----------------------------------------------------------------------


\bigskip
%bib files are not available.-------------------------
\bibliographystyle{amsplain}
\begin{thebibliography}{0}

%ref-book----------------------------------------------
\bibitem{bc} 
C. Baiocchi and A. Capelo, 
{\it Variational and Quasi Variational Inequalities}, 
J. Wiley and Sons, 
New York, 
1984.

%ref-journal-------------------------------------------
\bibitem{harrow2009}
A. W. Harrow, A. Hassidim and S. Lloyd,
{\it Quantum algorithm for linear Systems of equations},
Phys. Rev. Lett
{\bf 103} (2009),
150502.

\bibitem{aaronson2015}
S. Aaronson,
{\it Read the fine print},
Nature Phys
{\bf 11} (2015),
291-293.

\bibitem{kerenidis2017}
I. Kerenidis and A. Prakash,
{\it Quantum Recommendation Systems},
ITCS
{\bf 67} (2017),
49:1--49:21.

\bibitem{babbush2018}
R. Babbush, C. Gidney, B. Dominic, N. Weibe, J. McClean, A. Paler, A. Fowler
and H. Neven,
{\it Encoding Electronic Spectra in Quantum Circuits with Linear $T$ Complexity}
Phys. Rv.
{\bf 8} (2018),
041015.

\bibitem{chakraborty2019}
Chakraborty, Gily{\'en} and Jeffery. {\it The Power of Block-Encoded Matrix
Powers}, ICALP {\bf 132} (2019), 33:1-33:14.

\bibitem{tang2019}
E. Tang. {\it A Quantum-Inspired Classical Algorithm for Recommendation Systems},
Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing (2019),
217--228.

\bibitem{kerenidis2020}
Kerenidis, Iordanis and Prakash, Anupam. {\it Quantum gradient descent for
linear systems and leaste squares}, Phys.Rev.A {\bf 2} (2020), 022316.

\bibitem{jaques2023}
S. Jaques and A. Rattew,
{\it QRAM: A Survey and Critique},
arXiv: 2305.10310 [quant-ph] (2023).

\bibitem{tang2023}
E. Tang,
{\it Quantum Machine Learning Without Any Quantum},
Ph.D. diss, University of Washington (2023).

\bibitem{cp} 
D. Chan and J.S. Pang, 
{\it The generalized quasi variational inequality problems}, 
Math. Oper. Research 
{\bf 7} (1982), 
211-222.

\bibitem{belly} 
C. Belly, 
{\it Variational and Quasi Variational Inequalities}, 
J. Appl. Math. and Computing 
{\bf 6} (1999), 
234-266.

\bibitem{pang} 
D. Pang, 
{\it The generalized quasi variational inequality problems}, 
J. Appl. Math. and Computing 
{\bf 8} (2002), 
123-245.



\bigskip
\bigskip

%Type a brief sketch of biography and research interests not exceeding 85 words.------------------------
%The following is a sample biography

\pn {\bf 1st Author name} received M.Sc. from Seoul National University and Ph.D. at University of Minnesota. 
Since 1992 he has been at Chungnam National University. 
His research interests include numerical optimization and biological computation.

\medskip
\pn % Affiliations / Addresses
Department of Mathematics, Chungnam National University, Daejeon 305-764, Korea. 
\pn{\tt e-mail: soh@cnu.ac.kr}

\bigskip
\pn {\bf 2nd Author name} received M.Sc. from Kyungpook National University, and Ph.D. from Iowa State University. 
He is currently a professor at Chungbuk National University since 1991. 
His research interests are computational mathematics, iterative method and parallel computation.

\medskip
\pn % Affiliations / Addresses
Department of Mathematics, College of Natural Sciences, Chungbuk National University, Cheongju 361-763, Korea.
\pn{\tt e-mail: gmjae@chungbuk.ac.kr}



\end{thebibliography}

\end{document}
