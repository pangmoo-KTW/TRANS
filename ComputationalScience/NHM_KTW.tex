\documentclass[hidelinks,article,a4paper,chapter,twocolumn]{oblivoir}
\usepackage{lipsum}
\usepackage{fapapersize}
\usefapapersize{210mm,297mm,30mm,*,30mm,32mm}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{braket,graphicx,xcolor,caption}
\usepackage{enumitem,mdframed,ob-chapstyles}
\newtheorem{defn}{정의}[chapter]
\newtheorem{theo}{정리}[chapter]
\newtheorem{exem}{예시}[chapter]
\newtheorem{lemm}{보조정리}[chapter]
\newtheorem{thes}{논제}[chapter]
\newtheorem{axio}{공리}[chapter]
\newtheorem{prob}{문제}[chapter]
% chapterstyle
\chapterstyle{obkomalike}
% header & footer
\makepagestyle{mystyle} 
\makeevenhead{mystyle}{\thepage}{국립부경대학교}{}
\makeoddhead{mystyle}{}{블록 인코딩의 구성}{\thepage}
\pagestyle{mystyle}
% reference setting
\renewcommand\bibname{\centering References}
\let\oldbibliography\bibliography
\renewcommand{\bibliography}[1]{{%
\let\chapter\section
\oldbibliography{#1}}}
% dagger thanks
\makeatletter
\def\@fnsymbol#1{\ensuremath{\ifcase#1\or \dagger\or \dagger\dagger\or
\mathsection\or \mathparagraph\or \|\or **\or \dagger\dagger
\or \dagger\dagger \else\@ctrerr\fi}}
\makeatother
% keywords command
\providecommand{\keywords}[1]
{
  \textbf{\textit{Keywords:}} #1
}
\providecommand{\kkeywords}[1]
{
  \textbf{{키워드:}} #1
}
% abstract align
\renewenvironment{abstract}
{\textbf{\abstractname.}\ \ignorespaces}
 {\par\medskip}
 % title & author
\title{\Huge\textbf{블록 인코딩의 구성\\}\huge(Construction of Block-encodings)}
\author{
  \begin{tabular}{cc}
    \shortstack{\emph{\Large노\;\;현\;\;민\;\;}\thanks{국립부경대학교 과학컴퓨팅학과 학생}}
 &
    \shortstack{{\emph{\Large김\;\;태\;\;원\;}}\thanks{국립부경대학교 컴퓨터공학과 학생}}\\
    \shortstack{(Hyeonmin Roh)} & \shortstack{(Taewon Kim)}
  \end{tabular}
}
\date{}
% //////////////
% BEGIN DOCUMENT
\begin{document}
\pagenumbering{gobble}    % Title Page
\onecolumn 
\maketitle\thispagestyle{empty}
\begin{abstract}
  \emph{블록 인코딩}(Block-encoding)은 양자 기계학습 및 선형대수 알고리즘이 
  제 고전 판본에 대해 보이는 지수적인 속도 증진의 근간인 양자 자료구조, 이른바
  QRAM을 구성하는 도구다. 한편 일부 양자 기계학습 및 선형대수 알고리즘들을 오직
  다항 시간 감속만으로 고전 알고리즘에 의해  모사할 수도 있는데, 이런 모사를 
  역양자화(Dequantization)라고 부른다. 또한 역양자화란 QRAM을 구성하는 블록
  인코딩의 대수적인 성질에 주목한 결과이기도 하다. 따라서 본고는 역양자화를
  방지하려면 블록 인코딩을 어떻게 구성할 것이냐는 문제를 중심으로 블록 인코딩과
  인접 개념을 일별한다. 
\end{abstract}
\kkeywords{양자 기계학습, 양자 알고리즘, 블록 인코딩, 양자 특잇값
변환}
\hfill\break

\renewcommand{\abstractname}{\textit{Abstract}}
\begin{abstract}
  \emph{Block-encoding} is a tool that constructs the quantum data structure
  known as QRAM, which serves as the foundation for the exponential speedup
  observed in quantum machine learning and linear algebra algorithms compared
  to their classical counterparts. On the other hand, certain quantum machine
  learning and linear algebra algorithms can be emulated classicaly, in other
  words, dequantized, with only polynomial-time slowdown.
  Dequantization is, in part, a result of exploiting the algebraic properties
  of block-encoding that make up QRAM. Therefore, this paper sketches the
  notion of block encoding and related concepts, with a central focus
  on the problem of how to construct it to prevent dequantization.
\end{abstract}
\keywords{quantum machine learning, quantum algorithms,
block-encoding, QSVT}
\clearpage 
\twocolumn            
\pagenumbering{arabic}    % Main Matter
\chapter{개요}
\section{역사}
양자 기계학습(Quantum Machine Learning) 알고리즘은 고전 기계학습 알고리즘에
대해 지수적인 가속(exponential speedup)을 보인다는 주장이 존재한다. 이들
주장과 더불어 양자 기계학습이라는 분야 자체가 연립일차방정식 $Ax=b$의 해를
구하는 HHL 알고리즘\cite{HHL2009}에 강하게 의존하는데, 일찍이 
Aaronson\cite{Aaronson2015}은 HHL 알고리즘과 양자 기계학습 및 그 이점이
지니는 한계를 지적했다. 머지 않아 Tang\cite{Tang2019}은 기계학습에서 주로
사용하는 고전 데이터에 대해 오직 다항 수준의 감속(polynomial slowdown)으로
양자 추천 알고리즘\cite{KP2017}을 모사하는 고전 알고리즘을 내놓으며 이런 과정을
역양자화(dequantization)라고 명명했다.
\section{표기 및 기초}
\chapter{역양자화}
\section{양자 기계학습과 고전 데이터}
\section{QSVT}
\chapter{블록 인코딩}
\section{기초 정의}
블록 인코딩은 유니타리가 해밀토니언 시뮬레이션의 효과적인 해법에 관한
Childs et.al \cite{Childs2017}의 연구에서 비롯하며, 기본적으로는 유니타리가
아닌 행렬을 양자컴퓨터에서 사용할 수 있도록 하는 기법이다. 

행렬 $A\in\mathbb{C}^{r\times c}$가 있다고 하자. 이때 $A$는 정방일 필요가
없다. 그렇다면 유니타리 $U$가 존재하여 $A$가 $U$의 좌측 상단을 차지한다. 즉
임의의 행렬 성분 $\cdot$ 에 대해 $A$의 블록 인코딩 $U$는 아래와 같다.
\begin{equation}
  U = \begin{bmatrix}A&\cdot\\\cdot&\cdot\end{bmatrix}
\end{equation}
여기서 $A$의 지수 $r,c$를 $2$의 거듭제곱으로 둬, 아래처럼 $\ket0$과 $\bra0$에
대해 나타내는 것이 목표다. 
\begin{equation}
  A = (\ket0^{\otimes a_L}\otimes I)U(\ket0^{\otimes a_R}\otimes I)
\end{equation}
$r,c$가 $2$의 거듭제곱이어야 하는 것은 큰 제약이 아니다. $A$에 대해 $A_e\in
\mathbb{C}^{2^s}\times{2^s}$를 정의하여 $A$를 $A_e$의 좌측 상단에 두고 나머지
성분은 $0$으로 두면 되기 때문이다. 

일반적으로 블록 인코딩은 \cite{Gilyen2019, Rall2020} 아래처럼 정의한다.
\begin{defn}[(블록 인코딩)]
  $A$, 조정자 $\alpha\in\mathbb{R}_{+}$, 블록을 위한 패딩 $a\in\mathbb{R}_+$,
  정밀도 $\epsilon\in\mathbb{R}_+$가 주어질 때, $U$가 아래를 만족하며
  \begin{equation}
    \|A/\alpha-(\bra0^{\otimes a}\otimes I)U(\ket0^{\otimes a}\otimes I)\|\leq
    \epsilon
  \end{equation}
  $Q$개의 게이트로 구현할 수 있다면 $\epsilon$ 정밀도의 $\alpha$ 조정
  $Q$ 블록 인코딩이라고 부른다.
\end{defn}
의미를 더 직관적으로 소화하기 위해 $\epsilon$과 $\alpha$를 생략하여
정확하며 $1$로 조정된 $Q$ 블록 인코딩을 정의하면 아래와 같다.
\begin{defn}
  $A\in\mathbb{C}^{r\times c}$가 주어질 때, $U\in\mathbb{C}^{d\times d}$는
  $U$가 $\mathcal{O}(Q)$ 게이트로 구현가능하고 항등행렬의 첫 $r$열과 $c$열인
  $B_{L}\in\mathbb{C}^{d\times r},B_{R}\in\mathbb{C}^{d\times c}$에 대해
  \begin{equation}\label{eq:1-1}
    B_{L}^{\dagger}UB_{R} = A
  \end{equation}
  를 만족하면 $A$의 $Q$ 블록 인코딩이라고 부른다. 
\end{defn}
아래는 이들 정의가 동치라는 사실을 이해하기 위한 예시다.
\begin{exem}
$c=2,d=8,r=4$라고 가정하자. 다시 말해 $A\in\mathbb{C}^{4\times2},
U\in\mathbb{C}^{8\times 8}, B_{L,1}\in\mathbb{C}^{8\times 4}, 
B_{R,1}\in\mathbb{C}^{8\times 2}$라고 가정하자.
\begingroup
\setlength\arraycolsep{-0.1pt}
\begin{align*}
  &B^{\dagger}_{L} =
    \begin{pmatrix}
      1&0&0&0&0&0&0&0\\0&1&0&0&0&0&0&0\\0&0&1&0&0&0&0&0\\0&0&0&1&0&0&0&0
  \end{pmatrix},
  B_{R} =  \begin{pmatrix}
    1&0\\0&1\\0&0\\0&0\\0&0\\0&0\\0&0\\0&0
  \end{pmatrix}\\
  &U = \begin{pmatrix}A&\cdot\\\cdot&\cdot\end{pmatrix} =
  \begin{pmatrix}
    a_{11}&a_{12}&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
    a_{21}&a_{22}&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
    a_{31}&a_{32}&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
    a_{41}&a_{42}&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
    \cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
    \cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
    \cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
    \cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot
  \end{pmatrix},\\
  &\Rightarrow\quad B_{L}^{\dagger}U = 
  \begin{pmatrix}
    a_{11}&a_{12}&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
    a_{21}&a_{22}&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
    a_{31}&a_{32}&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot\\
    a_{41}&a_{42}&\cdot&\cdot&\cdot&\cdot&\cdot&\cdot
  \end{pmatrix}\\&\Rightarrow
  B_{L}^{\dagger}UB_{R} =
  \begin{pmatrix}
    a_{11}&a_{12}\\a_{21}&a_{22}\\a_{31}&a_{32}\\a_{41}&a_{42}
  \end{pmatrix}
= A
\end{align*}
\endgroup
\end{exem}
\bibliography{ref}
\bibliographystyle{plain}
\end{document}
